{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Project — Beta Bank Churn Prediction\n",
    "# --------------------------------------------------------\n",
    "#\n",
    "# Goal:\n",
    "#     Predict whether a customer will leave Beta Bank (binary classification).\n",
    "#     Target metric: F1 score ≥ 0.59 on the test set.\n",
    "#     Also evaluate AUC-ROC to compare against F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing what I think is needed to make a valid model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0     2.0       0.00              1          1               1   \n",
      "1     1.0   83807.86              1          0               1   \n",
      "2     8.0  159660.80              3          1               0   \n",
      "3     1.0       0.00              2          0               0   \n",
      "4     2.0  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load and View data\n",
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns (RowNumber, CustomerId, Surname) are identifiers and not predictive,\n",
    "# so we remove them to prevent noise or data leakage.\n",
    "data = data.drop(['RowNumber','CustomerId','Surname'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features/target\n",
    "features = data.drop('Exited', axis = 1)\n",
    "target = data['Exited']\n",
    "\n",
    "# Stratified splitting ensures the proportion of churners vs non-churners\n",
    "# is preserved across train/validation/test sets, which is important for imbalanced data.\n",
    "# Without stratify, we could end up with splits where the minority class\n",
    "# (Exited = 1) is underrepresented or even missing in validation/test,\n",
    "# which would make evaluation unreliable.\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target, random_state=12345\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=12345\n",
    ")\n",
    "# 60/20/20 split\n",
    "\n",
    "# Make copies to avoid SettingWithCopyWarning\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "X_test  = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing:\n",
    "# 1. Missing values handled: categorical -> 'Unknown', numeric -> median.\n",
    "# 2. One-hot encoding: expanded categorical columns into binary indicators.\n",
    "# 3. Scaling: standardized numeric features to mean=0, std=1 for fair treatment by models.\n",
    "# Categorical and numeric columns\n",
    "categorical = ['Geography', 'Gender']\n",
    "numeric = ['CreditScore','Age','Tenure','Balance','NumOfProducts',\n",
    "           'HasCrCard','IsActiveMember','EstimatedSalary']\n",
    "\n",
    "# --- 1. Handle missing values BEFORE OHE ---\n",
    "for col in categorical:\n",
    "    X_train[col] = X_train[col].fillna('Unknown')\n",
    "    X_valid[col] = X_valid[col].fillna('Unknown')\n",
    "    X_test[col]  = X_test[col].fillna('Unknown')\n",
    "\n",
    "for col in numeric:\n",
    "    median = X_train[col].median()  # use training set median\n",
    "    X_train[col] = X_train[col].fillna(median)\n",
    "    X_valid[col] = X_valid[col].fillna(median)\n",
    "    X_test[col]  = X_test[col].fillna(median)\n",
    "\n",
    "# --- 2. One-Hot Encoding ---\n",
    "X_train = pd.get_dummies(X_train, columns=categorical, drop_first=True)\n",
    "X_valid = pd.get_dummies(X_valid, columns=categorical, drop_first=True)\n",
    "X_test  = pd.get_dummies(X_test,  columns=categorical, drop_first=True)\n",
    "\n",
    "# Align columns across sets\n",
    "X_valid = X_valid.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test  = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# --- 3. Scale numeric features ---\n",
    "# Scaling helps models like Logistic Regression converge faster\n",
    "# and prevents features with large scales (like Balance or EstimatedSalary)\n",
    "# from dominating those with smaller ranges (like NumOfProducts).\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric] = scaler.fit_transform(X_train[numeric])\n",
    "X_valid[numeric] = scaler.transform(X_valid[numeric])\n",
    "X_test[numeric]  = scaler.transform(X_test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in target (Exited):\n",
      "0    0.7963\n",
      "1    0.2037\n",
      "Name: Exited, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Examine class balance\n",
    "print(\"Class distribution in target (Exited):\")\n",
    "print(target.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that about ~80% of customers did not exit while only ~20% did.\n",
    "# This imbalance means accuracy is a misleading metric\n",
    "# a model predicting \"no churn\" for everyone would already be ~80% accurate\n",
    "# while failing to capture churners. That’s why F1 and AUC-ROC are better suited here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression:\n",
      "F1: 0.3214953271028037\n",
      "AUC-ROC: 0.7874854824007367\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (Baseline) model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=12345)\n",
    "logreg.fit(X_train, y_train)\n",
    "proba_valid = logreg.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('Baseline Logistic Regression:')\n",
    "print('F1:', f1_score(y_valid, (proba_valid > 0.5).astype(int)))\n",
    "print('AUC-ROC:', roc_auc_score(y_valid, proba_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results:\n",
    "# A simple Logistic Regression was trained without any imbalance correction.\n",
    "# It achieved an F1 score of ~0.32, reflecting poor capture of the minority class,\n",
    "# even though the AUC-ROC (~0.79) suggests the model separates the classes\n",
    "# reasonably well in terms of probability. This shows the imbalance issue:\n",
    "# probability ranking looks fine, but binary decisions underperform for recall/precision.\n",
    "# This step establishes a performance floor (baseline).\n",
    "# Any further methods (balancing, ensembles, tuning) must beat this to justify their complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Balanced):\n",
      "F1: 0.5108601216333623\n",
      "AUC-ROC: 0.7917516900567748\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (Baseline) model\n",
    "logreg_bal = LogisticRegression(max_iter=1000, random_state=12345, class_weight='balanced')\n",
    "logreg_bal.fit(X_train, y_train)\n",
    "proba_valid_bal = logreg_bal.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('Logistic Regression (Balanced):')\n",
    "print('F1:', f1_score(y_valid, (proba_valid_bal > 0.5).astype(int)))\n",
    "print('AUC-ROC:', roc_auc_score(y_valid, proba_valid_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This comparison shows that the improvement is indeed due to handling imbalance.\n",
    "# The plain Logistic Regression had F1 ≈ 0.32, while the balanced version reached F1 ≈ 0.51.\n",
    "# Since the AUC-ROC stayed similar, we can conclude that class weighting improved F1\n",
    "# by giving more attention to the minority class.\n",
    "# This shows that re-weighting shifted the decision boundary,\n",
    "# catching more positives at the cost of slightly lower precision,\n",
    "# which is exactly what we want for churn detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (Plain):\n",
      "F1: 0.5513866231647634\n",
      "AUC-ROC: 0.8713289560747188\n"
     ]
    }
   ],
   "source": [
    "# Random Forest (Plain, no imbalance fix)\n",
    "rf_plain = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=12345\n",
    ")\n",
    "rf_plain.fit(X_train, y_train)\n",
    "proba_valid_plain = rf_plain.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(\"\\nRandom Forest (Plain):\")\n",
    "print(\"F1:\", f1_score(y_valid, (proba_valid_plain > 0.5).astype(int)))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_valid, proba_valid_plain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "F1: 0.6386554621848739\n",
      "AUC-ROC: 0.8673326639428335\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=12345\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "proba_valid = rf.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(\"F1:\", f1_score(y_valid, (proba_valid > 0.5).astype(int)))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_valid, proba_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing plain vs balanced Random Forest confirms that\n",
    "# imbalance handling (via class_weight) directly improves recall/F1,\n",
    "# while AUC-ROC remains roughly the same — echoing the pattern seen in Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting:\n",
      "F1: 0.5750000000000001\n",
      "AUC-ROC: 0.8797480068666509\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state=12345)\n",
    "gb.fit(X_train, y_train)\n",
    "proba_valid = gb.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(\"\\nGradient Boosting:\")\n",
    "print(\"F1:\", f1_score(y_valid, (proba_valid > 0.5).astype(int)))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_valid, proba_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findings:\n",
    "# - Random Forest performed best: F1 ~0.64 and AUC-ROC ~0.87, comfortably\n",
    "#   above the project threshold of F1 ≥ 0.59. It effectively balances recall\n",
    "#   and precision by leveraging class weighting.\n",
    "# - Gradient Boosting achieved F1 ~0.58 and AUC-ROC ~0.88, meaning it had\n",
    "#   stronger ranking power (AUC-ROC) but fell short on F1. This suggests it\n",
    "#   leaned more towards precision than recall under the 0.5 threshold.\n",
    "#\n",
    "# Thus, Random Forest was chosen as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (Upsampled):\n",
      "F1: 0.6225596529284165\n",
      "AUC-ROC: 0.8655111197484079\n"
     ]
    }
   ],
   "source": [
    "X_train_upsampled, y_train_upsampled = resample(\n",
    "    X_train[y_train==1],\n",
    "    y_train[y_train==1],\n",
    "    replace=True,\n",
    "    n_samples=y_train[y_train==0].shape[0],\n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "X_train_up = pd.concat([X_train[y_train==0], X_train_upsampled])\n",
    "y_train_up = pd.concat([y_train[y_train==0], y_train_upsampled])\n",
    "\n",
    "# Shuffle to mix the classes\n",
    "X_train_up, y_train_up = resample(X_train_up, y_train_up, random_state=12345)\n",
    "\n",
    "# Train Random Forest on upsampled data\n",
    "rf_up = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=12345\n",
    ")\n",
    "rf_up.fit(X_train_up, y_train_up)\n",
    "proba_valid_up = rf_up.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(\"\\nRandom Forest (Upsampled):\")\n",
    "print(\"F1:\", f1_score(y_valid, (proba_valid_up > 0.5).astype(int)))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_valid, proba_valid_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling is a second imbalance fix.\n",
    "# Unlike class_weight (which just re-weights), this physically duplicates minority examples\n",
    "# to balance the training set. It can sometimes cause overfitting,\n",
    "# but it gives the model more exposure to churn patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from search: {'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 10, 'class_weight': 'balanced'}\n",
      "Best F1 score from search: 0.5976878675347547\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=12345),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    random_state=12345,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters from search:\", rf_search.best_params_)\n",
    "print(\"Best F1 score from search:\", rf_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning helps avoid arbitrary parameter choices\n",
    "# and finds combinations that maximize F1. RandomizedSearchCV is used instead of GridSearch\n",
    "# because it’s faster and still explores the search space effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results:\n",
      "F1: 0.6396292004634994\n",
      "AUC-ROC: 0.8726677370745167\n"
     ]
    }
   ],
   "source": [
    "# Use the Random Forest model selected from validation\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    min_samples_split=5,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "# Train on the combined training+validation data for maximum learning\n",
    "# Retraining on train+valid ensures the final model\n",
    "# uses all available labeled data before the test set, which I believe is common best practice.\n",
    "rf_final.fit(pd.concat([X_train, X_valid]), pd.concat([y_train, y_valid]))\n",
    "\n",
    "# Predict probabilities and convert to binary predictions\n",
    "proba_test = rf_final.predict_proba(X_test)[:, 1]\n",
    "pred_test = (proba_test > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Final Test Results:\")\n",
    "print(\"F1:\", f1_score(y_test, pred_test))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test, proba_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Testing ---\n",
    "# We performed the final evaluation using the held-out test set to check how well the best model\n",
    "# (Random Forest with class balancing) generalizes to unseen data. \n",
    "# Results:\n",
    "#   • F1 Score: ~0.64 → comfortably above the project threshold of 0.59, showing that the model\n",
    "#     achieves a solid balance between precision and recall in predicting churn.\n",
    "#   • AUC-ROC: ~0.872 → indicates the model is excellent at ranking customers by their churn risk,\n",
    "#     with performance far above a random baseline (0.5).\n",
    "#\n",
    "# Together, these metrics confirm the Random Forest model is robust, generalizes well to new data,\n",
    "# and provides reliable predictive performance for customer churn detection.\n",
    "\n",
    "# --- Project Wrap-Up ---\n",
    "# In summary:\n",
    "#   1. Data was cleaned, missing values handled, categorical features encoded, and numeric features scaled.\n",
    "#   2. We identified a strong class imbalance (≈80% non-churn vs 20% churn) and tested multiple fixes:\n",
    "#      • Baseline models (Logistic Regression, Random Forest, Gradient Boosting)\n",
    "#      • Class weighting\n",
    "#      • Upsampling of the minority class\n",
    "#   3. Comparisons showed that imbalance handling significantly boosted F1 scores while AUC-ROC remained stable.\n",
    "#   4. Hyperparameter tuning (RandomizedSearchCV) further improved Random Forest performance.\n",
    "#\n",
    "# Final Outcome: Random Forest with class weighting was chosen as the best model, achieving the project’s\n",
    "# target F1 score and demonstrating reliable predictive power. The process shows how imbalance corrections,\n",
    "# model comparisons, and tuning are critical steps in building practical supervised learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
