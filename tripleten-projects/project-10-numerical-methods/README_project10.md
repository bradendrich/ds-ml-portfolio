
# Numerical Methods â€” Rusty Bargain Car Price Prediction

## ğŸ“Œ Project Overview
Rusty Bargain, a used car sales service, wants to build an app that instantly estimates car prices based on technical specifications and listing information.  
Your task is to:

1. **Explore and preprocess the dataset**
2. **Train multiple machine learning models**
3. **Compare prediction quality, training speed, and inference time**
4. **Select the best-performing model using RMSE**

This project introduces numerical optimization, model benchmarking, and high-performance gradient boosting libraries.

---

## ğŸ“ Dataset Description
Source: `/datasets/car_data.csv`

### Features:
- `DateCrawled` â€” date the profile was downloaded  
- `VehicleType` â€” vehicle body type  
- `RegistrationYear` â€” year of vehicle registration  
- `Gearbox` â€” transmission type  
- `Power` â€” horsepower  
- `Model` â€” car model  
- `Mileage` â€” odometer reading (km)  
- `RegistrationMonth` â€” month of registration  
- `FuelType` â€” fuel type  
- `Brand` â€” manufacturer  
- `NotRepaired` â€” 'yes' or 'no' for unrepaired damage  
- `DateCreated` â€” listing creation date  
- `NumberOfPictures` â€” number of photos  
- `PostalCode` â€” seller postal code  
- `LastSeen` â€” last user activity  

### Target:
`Price` â€” vehicle price in Euros  

---

## ğŸ§¹ Step 1 â€” Data Preparation
The notebook includes:

- Removal of unrealistic values (e.g., registration year < 1900, power 0)  
- Converting dates into numeric/cyclical representations  
- One-hot encoding for categorical features when needed  
- Train-test split  

Data cleaning ensures stable model training and prevents numeric instability.

---

## ğŸ¤– Step 2 â€” Model Training

The project compares the following models:

### âœ” Linear Regression (Sanity Check)
- Extremely fast to train  
- Helps verify that boosting models perform **better**  
- RMS error used as a baseline  

### âœ” Decision Tree Regressor
- Tuned for `max_depth`, `min_samples_split`  
- Risk of overfitting but good for establishing non-linearity  

### âœ” Random Forest Regressor
- Tuned `n_estimators`, `max_depth`  
- More stable predictions  
- Slower training than single tree  

### âœ” LightGBM (Gradient Boosting)
- Handles categorical features natively  
- Very fast training  
- High-quality predictions  
- Tuned with:
  - `num_leaves`
  - `learning_rate`
  - `n_estimators`

### âœ” CatBoost (optional)
- Handles categorical features extremely well  
- Good performance with minimal preprocessing  

### âœ” XGBoost (optional)
- Requires one-hot encoding  
- Competitive model quality  

---

## âš™ï¸ Step 3 â€” Performance Comparison

Metrics tracked:

### **1. RMSE (Quality)**  
Used as the primary evaluation metric:
\[
RMSE = \sqrt{rac{1}{n}\sum (y - \hat{y})^2}
\]

### **2. Training Time**  
Measured with Jupyter `%%time` magic.

### **3. Prediction Time**  
Useful for real-time value prediction in the mobile app.

### âœ” Key Findings:
- **LightGBM** and **CatBoost** deliver the best RMSE  
- **Linear Regression** is fast but inaccurate  
- **Random Forest** performs well but is slower  
- **Gradient boosting** provides the best tradeoff between speed & accuracy  

---

## ğŸ“ Final Conclusions
Your notebook concludes with:

- Best model selected using RMSE  
- Benchmark comparison across:
  - Prediction speed  
  - Training time  
  - Accuracy  
- Pros & cons of each algorithm  
- A clear justification for the recommended model  

---

## ğŸ—‚ Suggested Repository Structure

```
project11-numerical-methods/
â”‚â”€â”€ data/
â”‚â”€â”€ notebooks/
â”‚   â””â”€â”€ project11_numerical_clean.ipynb
â”‚â”€â”€ README.md
â”‚â”€â”€ CHANGELOG.md
â”‚â”€â”€ requirements.txt
```

---

## â–¶ï¸ Running the Project

```bash
pip install -r requirements.txt
jupyter notebook notebooks/project11_numerical_clean.ipynb
```

---

## ğŸ‘¤ Author
**Braden Richards**  
Machine Learning â€¢ Numerical Optimization â€¢ Model Benchmarking  
Focused on ML performance, speed optimization, and applied regression modeling.

